{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport glob\nfrom joblib import Parallel, delayed\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns',300)\npd.set_option('max_rows',500)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-15T03:19:44.841571Z","iopub.execute_input":"2021-08-15T03:19:44.842166Z","iopub.status.idle":"2021-08-15T03:19:45.051413Z","shell.execute_reply.started":"2021-08-15T03:19:44.842015Z","shell.execute_reply":"2021-08-15T03:19:45.050369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculation of Basic Characteristic Variables","metadata":{}},{"cell_type":"markdown","source":"### Calculate WAP\n$$ \\text{wap} = \\frac{\\text{bid_price} * \\text{ask_size} + \\text{ask_price} * \\text{bid_size}}{\\text{bid_size} + \\text{ask_size}} $$\n### Calculate log return\n$$ \\text{log return}_{t_1,t_2} = LR_{t_1,t_2} = \\log{\\frac{S_{t_2}}{S_{t_1}}}  = \\log{S_{t_2}} - \\log{S_{t_1}}$$ \n### Calculate realized volatility\n$$ \\text{realized volatility} = \\sigma = \\sqrt{\\sum_t{LR_{t-1,t}^2}} $$","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:20:45.719575Z","iopub.execute_input":"2021-08-10T07:20:45.720076Z","iopub.status.idle":"2021-08-10T07:20:45.72447Z","shell.execute_reply.started":"2021-08-10T07:20:45.720046Z","shell.execute_reply":"2021-08-10T07:20:45.723279Z"}}},{"cell_type":"code","source":"def calculate_wap(df,method):\n    if method == 1:\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    if method == 2:\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    series = log_return(series)\n    return np.sqrt(np.sum(series**2))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:46.4923Z","iopub.execute_input":"2021-08-15T03:19:46.492701Z","iopub.status.idle":"2021-08-15T03:19:46.501346Z","shell.execute_reply.started":"2021-08-15T03:19:46.492666Z","shell.execute_reply":"2021-08-15T03:19:46.500199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Count Unique\nTo count how many unique elements in the series","metadata":{}},{"cell_type":"code","source":"def count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:47.834005Z","iopub.execute_input":"2021-08-15T03:19:47.834399Z","iopub.status.idle":"2021-08-15T03:19:47.839032Z","shell.execute_reply.started":"2021-08-15T03:19:47.834367Z","shell.execute_reply":"2021-08-15T03:19:47.837733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Consideration","metadata":{}},{"cell_type":"markdown","source":"### 1. The realized volatility $\\sigma$ of the past 10 mins\nUse the wap to calculate log return, then we can calculate $\\sigma$  \n**Be aware:** each time record(**seconds_in_bucket**) will have a **wap**, **log return** corresponding to it, while each **volatility $\\sigma$** only corresponds to one **time_id**\n### 2. Statistics(sum, mean, std) of some distribution information\n(1) **wap1**\n$$ \\text{wap1} = \\frac{\\text{bid_price}_1 * \\text{ask_size}_1 + \\text{ask_price}_1 * \\text{bid_size}_1}{\\text{bid_size}_1 + \\text{ask_size}_1} $$\n(2) **wap2**\n$$ \\text{wap2} = \\frac{\\text{bid_price}_2 * \\text{ask_size}_2 + \\text{ask_price}_2 * \\text{bid_size}_2}{\\text{bid_size}_2 + \\text{ask_size}_2} $$\n<font color=red>**wap1** and **wap2** are the highest and second highest stock Weighted Average Price </font>  \n<font color=blue>You can find the relevant information and evidence in **Figure 1** </font>  \n(3) **log_return1**\n$$ \\text{log_return}_{t_1,t_2}^{(1)} = \\log{\\frac{\\text{wap1}_{t_2}}{\\text{wap1}_{t_1}}}  = \\log{\\text{wap1}_{t_2}} - \\log{\\text{wap1}_{t_1}}$$ \n(4) **log_return2**\n$$ \\text{log_return}_{t_1,t_2}^{(2)} = \\log{\\frac{\\text{wap2}_{t_2}}{\\text{wap2}_{t_1}}}  = \\log{\\text{wap2}_{t_2}} - \\log{\\text{wap2}_{t_1}}$$ \n<font color=red>If the volatility $\\sigma$ is high, we can believe **log_return1** and **log_return2** will have high variance </font>  \n<font color=blue>You can find the relevant information and evidence in **Figure 2** </font>    \n(5) **wap_balance**\n$$ \\text{wap_balance} = |\\text{wap1} - \\text{wap2}| $$\n(6) **price_spread**\n$$ \\text{price_spread} = \\frac{\\text{ask_price}_1 - \\text{bid_price}_1}{\\frac{\\text{ask_price}_1 + \\text{bid_price}_1}{2}} \n= \\frac{2(\\text{ask_price}_1 - \\text{bid_price}_1)}{\\text{ask_price}_1 + \\text{bid_price}_1}$$\n(7) **bid_spread**\n$$ \\text{bid_spread} = \\text{bid_price}_1 - \\text{bid_price}_2 $$\n(8) **ask_spread**\n$$ \\text{ask_spread} = \\text{ask_price}_1 - \\text{ask_price}_2 $$\n<font color=red>A stock has high **volatility $\\sigma$** may have a big gap in **wap_balance, price_spread, bid_spread, ask_spread**</font>  \n<font color=blue>You can find the relevant information and evidence in **Figure 3** </font>  \n(9) **total_volume**\n$$ \\text{total_volume} = \\text{ask_size}_1 + \\text{ask_size}_2 + \\text{bid_size}_1 + \\text{bid_size}_2 $$\n(10) **volume_imbalance**\n$$ \\text{volume_imbalance} = |(\\text{ask_size}_1 + \\text{ask_size}_2) - (\\text{bid_size}_1 + \\text{bid_size}_2)| $$\n<font color=blue>You can find the relevant information and evidence in **Figure 4** </font>  \n### 3. Consider similar features in **Trade**\n(1) **realized_volatility**  \n(2) Statistics of **price, log_return, size, order_count**<font color=blue> (shown in **Figure 5**)</font>  \n(3) **count_unique** of **seconds_in_bucket**, **(That is, how many actual transactions have occurred in the trade)**  \n<font color=red>It can be considered that **a frequent transaction means higher volatility**</font>\n### 4. Handling **stock_id** and **time_id**\n(1) Use the **statistics** of historical **volatility $\\sigma$** corresponding to **stock_id** and **time_id** to represent their features  \n(2) Use **Target Encoding(TE)** to represent the feature of **stock_id, time_id**  \n(3) Use a **Embedding** layer in NN to generate feature vectors\n(4) Use **stock_id, time_id** as **category variables**\n### 5. Feature Scaling\nThis can eliminate the influence of some numerical fluctuations, but it may not be effective\n### 6. Time Series Analysis\nWe may use **historical time series** as features to directly predict **future series** or **future volatility**","metadata":{}},{"cell_type":"code","source":"stock_id = 0\nbook = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id='+str(stock_id))\nbook['wap1'] = calculate_wap(book,1)\nbook_sub = book.groupby(['time_id'])['wap1'].agg(realized_volatility).to_frame().reset_index()\nbook_sub.columns = ['time_id','realized volatility']\nbook_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:02:04.443443Z","iopub.execute_input":"2021-08-15T03:02:04.444356Z","iopub.status.idle":"2021-08-15T03:02:07.643889Z","shell.execute_reply.started":"2021-08-15T03:02:04.444299Z","shell.execute_reply":"2021-08-15T03:02:07.642854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fetch the first two times_id(time_id=5, 11) to compare the relationship between volatility $\\sigma$ and Features","metadata":{}},{"cell_type":"code","source":"book['wap2'] = calculate_wap(book,2)\nbook['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\nbook['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\nbook['wap_balance'] = abs(book['wap1'] - book['wap2'])\nbook['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\nbook['bid_spread'] = book['bid_price1'] - book['bid_price2']\nbook['ask_spread'] = book['ask_price1'] - book['ask_price2']\nbook['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\nbook['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\nbook_5 = book.loc[book['time_id']==5]\nbook_11 = book.loc[book['time_id']==11]\nbook_5.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:02:07.645539Z","iopub.execute_input":"2021-08-15T03:02:07.645824Z","iopub.status.idle":"2021-08-15T03:02:11.171229Z","shell.execute_reply.started":"2021-08-15T03:02:07.645797Z","shell.execute_reply":"2021-08-15T03:02:11.170195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Figure 1: **wap1** and **wap2**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(18,5))\naxes[0].plot(book_5['seconds_in_bucket'],book_5['wap1'],label='time_id:5')\naxes[0].plot(book_11['seconds_in_bucket'],book_11['wap1'],label='time_id:11')\naxes[0].set_xlabel('seconds_in_bucket')\naxes[0].set_ylabel('WAP')\naxes[0].set_title('WAP1')\naxes[0].legend()\naxes[1].plot(book_5['seconds_in_bucket'],book_5['wap2'],label='time_id:5')\naxes[1].plot(book_11['seconds_in_bucket'],book_11['wap2'],label='time_id:11')\naxes[1].set_xlabel('seconds_in_bucket')\naxes[1].set_ylabel('WAP')\naxes[1].set_title('WAP2')\naxes[1].legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:11.270252Z","iopub.execute_input":"2021-08-15T01:18:11.270538Z","iopub.status.idle":"2021-08-15T01:18:11.720653Z","shell.execute_reply.started":"2021-08-15T01:18:11.27051Z","shell.execute_reply":"2021-08-15T01:18:11.719519Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=red>From **Figure 1**, we can clearly see the correlation between **volatility $\\sigma$** and **wap1, wap2**. The transformation of WAP of stocks with low volatility is very gentle</font>","metadata":{}},{"cell_type":"markdown","source":"### Figure 2: **log_return1** and **log_return2**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(18,5))\naxes[0].plot(book_5['seconds_in_bucket'],book_5['log_return1'],label='time_id:5')\naxes[0].plot(book_11['seconds_in_bucket'],book_11['log_return1'],label='time_id:11')\naxes[0].set_xlabel('seconds_in_bucket')\naxes[0].set_ylabel('log_return')\naxes[0].set_title('log_return1')\naxes[0].legend()\naxes[1].plot(book_5['seconds_in_bucket'],book_5['log_return2'],label='time_id:5')\naxes[1].plot(book_11['seconds_in_bucket'],book_11['log_return2'],label='time_id:11')\naxes[1].set_xlabel('seconds_in_bucket')\naxes[1].set_ylabel('log_return')\naxes[1].set_title('log_return1')\naxes[1].legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:11.722296Z","iopub.execute_input":"2021-08-15T01:18:11.722707Z","iopub.status.idle":"2021-08-15T01:18:12.176624Z","shell.execute_reply.started":"2021-08-15T01:18:11.722679Z","shell.execute_reply":"2021-08-15T01:18:12.175367Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=red>From **Figure 2**, we can directly observe the performance of **volatility $\\sigma$** in **log_return**. Besides the value of $\\sigma$ in the past 10 mins, the statistics of log_return may be helpful</font>","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:43:26.324073Z","iopub.execute_input":"2021-08-10T09:43:26.324506Z","iopub.status.idle":"2021-08-10T09:43:26.331558Z","shell.execute_reply.started":"2021-08-10T09:43:26.324471Z","shell.execute_reply":"2021-08-10T09:43:26.329987Z"}}},{"cell_type":"markdown","source":"### Figure 3: **wap_balance, price_spread, bid_spread, ask_spread**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(2,2,figsize=(18,12))\naxes = axes.flatten()\nfeature_dict = ['wap_balance','price_spread','bid_spread','ask_spread']\nfor i in range(4):\n    axes[i].plot(book_5['seconds_in_bucket'],book_5[feature_dict[i]],label='time_id:5')\n    axes[i].plot(book_11['seconds_in_bucket'],book_11[feature_dict[i]],label='time_id:11')\n    axes[i].set_xlabel('seconds_in_bucket')\n    axes[i].set_ylabel(feature_dict[i])\n    axes[i].set_title(feature_dict[i])\n    axes[i].legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:12.178468Z","iopub.execute_input":"2021-08-15T01:18:12.1792Z","iopub.status.idle":"2021-08-15T01:18:13.142961Z","shell.execute_reply.started":"2021-08-15T01:18:12.179148Z","shell.execute_reply":"2021-08-15T01:18:13.141998Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=red>From **Figure 3**, we can find little correlation between **volatility $\\sigma$** and those four features. They show some trends, but not so strong. We can still add them to the features of the model. It seems that the stock with high **volatility** has high variance, extreme values, high gap in these four features</font>","metadata":{}},{"cell_type":"markdown","source":"### Figure 4: **total_volume** and **valume_inbalance**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,2,figsize=(18,5))\naxes[0].plot(book_5['seconds_in_bucket'],book_5['total_volume'],label='time_id:5')\naxes[0].plot(book_11['seconds_in_bucket'],book_11['total_volume'],label='time_id:11')\naxes[0].set_xlabel('seconds_in_bucket')\naxes[0].set_ylabel('total_volume')\naxes[0].set_title('total_volume')\naxes[0].legend()\naxes[1].plot(book_5['seconds_in_bucket'],book_5['volume_imbalance'],label='time_id:5')\naxes[1].plot(book_11['seconds_in_bucket'],book_11['volume_imbalance'],label='time_id:11')\naxes[1].set_xlabel('seconds_in_bucket')\naxes[1].set_ylabel('volume_imbalance')\naxes[1].set_title('volume_imbalance')\naxes[1].legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:13.144631Z","iopub.execute_input":"2021-08-15T01:18:13.145254Z","iopub.status.idle":"2021-08-15T01:18:13.534323Z","shell.execute_reply.started":"2021-08-15T01:18:13.145212Z","shell.execute_reply":"2021-08-15T01:18:13.533332Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=red>From **Figure 4**, we hardly see the correlations between **volatility $\\sigma$** and **total_volume, volume_imbalance**</font>","metadata":{}},{"cell_type":"code","source":"stock_id = 0\ntrade = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id='+str(stock_id))\ntrade.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:13.535812Z","iopub.execute_input":"2021-08-15T01:18:13.536115Z","iopub.status.idle":"2021-08-15T01:18:13.601997Z","shell.execute_reply.started":"2021-08-15T01:18:13.536083Z","shell.execute_reply":"2021-08-15T01:18:13.601307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_sub = trade.groupby(['time_id'])['price'].agg(realized_volatility).to_frame().reset_index()\ntrade_sub.columns = ['time_id','realized volatility']\ntrade_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:13.776729Z","iopub.execute_input":"2021-08-15T01:18:13.77735Z","iopub.status.idle":"2021-08-15T01:18:16.257554Z","shell.execute_reply.started":"2021-08-15T01:18:13.777312Z","shell.execute_reply":"2021-08-15T01:18:16.25651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade['log_return'] = trade.groupby(['time_id'])['price'].apply(log_return)\ntrade_5 = trade.loc[trade['time_id']==5]\ntrade_62 = trade.loc[trade['time_id']==62]\ntrade_5.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:16.259197Z","iopub.execute_input":"2021-08-15T01:18:16.259804Z","iopub.status.idle":"2021-08-15T01:18:17.90409Z","shell.execute_reply.started":"2021-08-15T01:18:16.25976Z","shell.execute_reply":"2021-08-15T01:18:17.902887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Figure 5: **price, log_return, size, order_count** in **Trade**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(2,2,figsize=(18,12))\naxes = axes.flatten()\nfeature_dict = ['price','log_return','size','order_count']\nfor i in range(4):\n    axes[i].plot(trade_5['seconds_in_bucket'],trade_5[feature_dict[i]],label='time_id:5')\n    axes[i].plot(trade_62['seconds_in_bucket'],trade_62[feature_dict[i]],label='time_id:62')\n    axes[i].set_xlabel('seconds_in_bucket')\n    axes[i].set_ylabel(feature_dict[i])\n    axes[i].set_title(feature_dict[i])\n    axes[i].legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:17.905778Z","iopub.execute_input":"2021-08-15T01:18:17.906105Z","iopub.status.idle":"2021-08-15T01:18:18.661584Z","shell.execute_reply.started":"2021-08-15T01:18:17.906074Z","shell.execute_reply":"2021-08-15T01:18:18.660338Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=red>From **Figure 5**, the difference of **volatility $\\sigma$** represented by these characteristics is not obvious. We can still use them to build the model, but there may be more key information hidden in **Trade**</font>","metadata":{}},{"cell_type":"markdown","source":"## Capture the features of the trend\nWe can use a window to select the **information closer to the future**, and obtain the **change trend of features** over time through multiple windows","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:00:04.470708Z","iopub.execute_input":"2021-08-10T11:00:04.471122Z","iopub.status.idle":"2021-08-10T11:00:04.475614Z","shell.execute_reply.started":"2021-08-10T11:00:04.471088Z","shell.execute_reply":"2021-08-10T11:00:04.474577Z"}}},{"cell_type":"code","source":"def get_stats_window(seconds_in_bucket, add_suffix = False):\n    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket]\n    # add seconds_in_bucket suffix\n    if add_suffix:\n        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:54.479855Z","iopub.execute_input":"2021-08-15T03:19:54.480473Z","iopub.status.idle":"2021-08-15T03:19:54.485646Z","shell.execute_reply.started":"2021-08-15T03:19:54.480423Z","shell.execute_reply":"2021-08-15T03:19:54.484511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Generation & Function Encapsulation","metadata":{}},{"cell_type":"markdown","source":"### Accelerate data processing with <font color=red>**numba**</font>","metadata":{}},{"cell_type":"code","source":"def calculate_wap(df,method):\n    if method == 1:\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    if method == 2:\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    return np.log(series).diff()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:55.461501Z","iopub.execute_input":"2021-08-15T03:19:55.461873Z","iopub.status.idle":"2021-08-15T03:19:55.470339Z","shell.execute_reply.started":"2021-08-15T03:19:55.461841Z","shell.execute_reply":"2021-08-15T03:19:55.468953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions used in **numba**","metadata":{}},{"cell_type":"code","source":"def realized_volatility_numba(values,index=None):\n    def log_return_in_realized_volatility(series):\n        return np.diff(np.log(series))\n    LR = log_return_in_realized_volatility(values)\n    return np.sqrt(np.sum(np.square(LR)))\n\ndef mean_numba(values,index=None):\n    return np.mean(values)\n\ndef sum_numba(values,index=None):\n    return np.sum(values)\n\ndef std_numba(values,index=None):\n    return np.std(values)\n\ndef min_numba(values,index=None):\n    return np.min(values)\n\ndef max_numba(values,index=None):\n    return np.max(values)\n\ndef max_min_numba(values,index=None):\n    return np.max(values) - np.min(values)\n\ndef count_unique_numba(values,index=None):\n    return len(np.unique(values))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:56.660361Z","iopub.execute_input":"2021-08-15T03:19:56.66076Z","iopub.status.idle":"2021-08-15T03:19:56.670887Z","shell.execute_reply.started":"2021-08-15T03:19:56.660723Z","shell.execute_reply":"2021-08-15T03:19:56.669476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Comparison","metadata":{}},{"cell_type":"markdown","source":"### <font color=red>**FeatureEngine** in traditional way</font>","metadata":{}},{"cell_type":"code","source":"def book_feature(file_path):\n    df = pd.read_parquet(file_path)\n    df['wap1'] = calculate_wap(df,1)\n    df['wap2'] = calculate_wap(df,2)\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    create_feature_dict = {\n        'wap1': [np.max, np.min, realized_volatility, np.mean, np.std],\n        'wap2': [np.max, np.min, realized_volatility, np.mean, np.std],\n        'log_return1': [np.max, np.min, np.mean, np.std],\n        'log_return2': [np.max, np.min, np.mean, np.std],\n        'wap_balance': [np.max, np.min, np.mean, np.std],\n        'price_spread':[np.max, np.min, np.mean, np.std],\n        'bid_spread':[np.max, np.min, np.mean, np.std],\n        'ask_spread':[np.max, np.min, np.mean, np.std],\n        'total_volume':[np.max, np.min, np.mean, np.std],\n        'volume_imbalance':[np.max, np.min, np.mean, np.std]\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    windows = []\n    seconds = [450, 300, 150]\n    time_id_list = ['time_id__'+str(second) for second in seconds]\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    for second in seconds:\n        windows.append(get_stats_window(seconds_in_bucket = second, add_suffix = True))\n    for i,second in enumerate(seconds):\n        df_feature = df_feature.merge(windows[i], how = 'left', left_on = 'time_id_', right_on = time_id_list[i])\n    df_feature.drop(time_id_list, axis = 1, inplace = True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\ndef trade_feature(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby(['time_id'])['price'].apply(log_return)\n    create_feature_dict = {\n        'price': [np.max, np.min, realized_volatility, np.mean, np.std],\n        'log_return': [np.max, np.min, np.mean, np.std],\n        'seconds_in_bucket': [count_unique],\n        'size': [np.max, np.min, np.mean, np.std],\n        'order_count': [np.max, np.min, np.mean, np.std]\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    windows = []\n    seconds = [450, 300, 150]\n    time_id_list = ['time_id__'+str(second) for second in seconds]\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    for second in seconds:\n        windows.append(get_stats_window(seconds_in_bucket = second, add_suffix = True))\n    for i,second in enumerate(seconds):\n        df_feature = df_feature.merge(windows[i], how = 'left', left_on = 'time_id_', right_on = time_id_list[i])\n    df_feature.drop(time_id_list, axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-15T03:19:58.154653Z","iopub.execute_input":"2021-08-15T03:19:58.155029Z","iopub.status.idle":"2021-08-15T03:19:58.186664Z","shell.execute_reply.started":"2021-08-15T03:19:58.154999Z","shell.execute_reply":"2021-08-15T03:19:58.185179Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=red>**FeatureEngine** with **numba** engine</font>","metadata":{}},{"cell_type":"code","source":"def book_feature_numba(file_path):\n    df = pd.read_parquet(file_path)\n    df['wap1'] = calculate_wap(df,1)\n    df['wap2'] = calculate_wap(df,2)\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df.fillna(0,inplace=True)\n    method_dict = {\n        'max': max_numba,\n        'min': min_numba,\n        'mean': mean_numba,\n        'std': std_numba\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        df_window = df[df['seconds_in_bucket'] >= seconds_in_bucket]\n        if add_suffix:\n            df_window = df_window.add_suffix('_' + str(seconds_in_bucket))\n        return df_window\n    \n    def get_feature(df_temp,time_id,suffix=None):\n        rv_list = ['wap1','wap2']\n        statistics_list = ['wap1','wap2','log_return1','log_return2',\n                           'wap_balance','price_spread','bid_spread','ask_spread',\n                           'total_volume','volume_imbalance']\n        if suffix != None:\n            rv_list = [x + '_' + str(suffix) for x in rv_list]\n            statistics_list = [x + '_' + str(suffix) for x in statistics_list]\n        df_feature = df_temp.groupby([time_id])[rv_list].agg(realized_volatility_numba,engine='numba').reset_index()\n        df_feature.columns = [col if col == time_id else col + '_rv' for col in df_feature.columns]\n        for method in method_dict:\n            merge_in = df_temp.groupby([time_id])[statistics_list].agg(method_dict[method],engine='numba').reset_index()\n            merge_in.columns = [col if col == time_id else (col + '_'+ method) for col in merge_in.columns]\n            df_feature = df_feature.merge(merge_in,how='left',left_on=time_id,right_on=time_id)\n        return df_feature\n    \n    windows = []\n    seconds = [450, 300, 150]\n    time_id_list = ['time_id_'+ str(second) for second in seconds] \n    df_all = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_all = get_feature(df_all,'time_id')\n    for second in seconds:\n        windows.append(get_stats_window(seconds_in_bucket = second, add_suffix = True))\n    for i,second in enumerate(seconds):\n        df_all = df_all.merge(get_feature(windows[i], time_id_list[i], second), \n                              how = 'left', left_on = 'time_id', right_on = time_id_list[i])\n    df_all.drop(time_id_list, axis = 1, inplace = True)\n    \n    stock_id = file_path.split('=')[1]\n    df_all['row_id'] = df_all['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_all.drop(['time_id'], axis = 1, inplace = True)\n    return df_all\n\ndef trade_feature_numba(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby(['time_id'])['price'].apply(log_return)\n    df.fillna(0,inplace=True)\n    method_dict = {\n        'max': max_numba,\n        'min': min_numba,\n        'mean': mean_numba,\n        'std': std_numba\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        df_window = df[df['seconds_in_bucket'] >= seconds_in_bucket]\n        if add_suffix:\n            df_window = df_window.add_suffix('_' + str(seconds_in_bucket))\n        return df_window\n    \n    def get_feature(df_temp,time_id,suffix=None):\n        rv_list = ['price']\n        statistics_list = ['price','log_return','size','order_count']\n        count_unique_list = ['seconds_in_bucket']\n        if suffix != None:\n            rv_list = [x + '_' + str(suffix) for x in rv_list]\n            statistics_list = [x + '_' + str(suffix) for x in statistics_list]\n            count_unique_list = [x + '_' + str(suffix) for x in count_unique_list]\n        df_feature = df_temp.groupby([time_id])[rv_list].agg(realized_volatility_numba,engine='numba').reset_index()\n        df_feature.columns = [col if col == time_id else col + '_rv' for col in df_feature.columns]\n        for method in method_dict:\n            merge_in = df_temp.groupby([time_id])[statistics_list].agg(method_dict[method],engine='numba').reset_index()\n            merge_in.columns = [col if col == time_id else (col + '_'+ method) for col in merge_in.columns]\n            df_feature = df_feature.merge(merge_in,how='left',left_on=time_id,right_on=time_id)\n        merge_in = df_temp.groupby([time_id])[count_unique_list].agg(count_unique_numba,engine='numba').reset_index()\n        merge_in.columns = [col if col == time_id else (col + '_count_unique') for col in merge_in.columns]\n        df_feature = df_feature.merge(merge_in,how='left',left_on=time_id,right_on=time_id)\n        return df_feature\n    \n    windows = []\n    seconds = [450, 300, 150]\n    time_id_list = ['time_id_'+str(second) for second in seconds] \n    df_all = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_all = get_feature(df_all,'time_id')\n    for second in seconds:\n        windows.append(get_stats_window(seconds_in_bucket = second, add_suffix = True))\n    for i,second in enumerate(seconds):\n        df_all = df_all.merge(get_feature(windows[i], time_id_list[i], second), \n                              how = 'left', left_on = 'time_id', right_on = time_id_list[i])\n    df_all.drop(time_id_list, axis = 1, inplace = True)\n    \n    df_all = df_all.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_all['row_id'] = df_all['trade_time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_all.drop(['trade_time_id'], axis = 1, inplace = True)\n    return df_all","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:58.902985Z","iopub.execute_input":"2021-08-15T03:19:58.903367Z","iopub.status.idle":"2021-08-15T03:19:58.941425Z","shell.execute_reply.started":"2021-08-15T03:19:58.903335Z","shell.execute_reply":"2021-08-15T03:19:58.94004Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def FeatureEngineering(list_stock_ids,is_train = True,use_numba=False):\n    def for_joblib(stock_id):\n        data_dir = '../input/optiver-realized-volatility-prediction/'\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # book\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n        if use_numba:\n            df_temp = pd.merge(book_feature_numba(file_path_book), \n                               trade_feature_numba(file_path_trade), how='left', on='row_id')\n        else:\n            df_temp = pd.merge(book_feature(file_path_book), \n                               trade_feature(file_path_trade), how='left', on='row_id')\n        return df_temp\n    \n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:19:59.52404Z","iopub.execute_input":"2021-08-15T03:19:59.524445Z","iopub.status.idle":"2021-08-15T03:19:59.532539Z","shell.execute_reply.started":"2021-08-15T03:19:59.524404Z","shell.execute_reply":"2021-08-15T03:19:59.531699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_stock_ids = train['stock_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:42.813516Z","iopub.execute_input":"2021-08-15T01:18:42.814131Z","iopub.status.idle":"2021-08-15T01:18:43.044761Z","shell.execute_reply.started":"2021-08-15T01:18:42.814095Z","shell.execute_reply":"2021-08-15T01:18:43.043507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_feature = FeatureEngineering(train_stock_ids, is_train = True, use_numba=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T14:01:59.676945Z","iopub.execute_input":"2021-08-10T14:01:59.677295Z","iopub.status.idle":"2021-08-10T14:30:55.114361Z","shell.execute_reply.started":"2021-08-10T14:01:59.677268Z","shell.execute_reply":"2021-08-10T14:30:55.113256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_feature_numba = FeatureEngineering(train_stock_ids, is_train = True, use_numba=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:18:50.224158Z","iopub.execute_input":"2021-08-15T01:18:50.224546Z","iopub.status.idle":"2021-08-15T01:30:11.070017Z","shell.execute_reply.started":"2021-08-15T01:18:50.224515Z","shell.execute_reply":"2021-08-15T01:30:11.068816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=red>From above, we can find **numba** engine can save more than 60% time in the process of feature generation</font>","metadata":{}},{"cell_type":"markdown","source":"### Save features for later use","metadata":{}},{"cell_type":"code","source":"train_feature_numba.to_csv('train_feature.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:33:46.271781Z","iopub.execute_input":"2021-08-15T01:33:46.27252Z","iopub.status.idle":"2021-08-15T01:37:26.477964Z","shell.execute_reply.started":"2021-08-15T01:33:46.272476Z","shell.execute_reply":"2021-08-15T01:37:26.476826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_feature_numba.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:37:26.479887Z","iopub.execute_input":"2021-08-15T01:37:26.480334Z","iopub.status.idle":"2021-08-15T01:37:26.48663Z","shell.execute_reply.started":"2021-08-15T01:37:26.480287Z","shell.execute_reply":"2021-08-15T01:37:26.485518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling **stock_id** and **time_id**","metadata":{}},{"cell_type":"markdown","source":"### (1) Use **statistics** of **volatility $\\sigma$** to represent the feature of **stock_id, time_id**","metadata":{}},{"cell_type":"code","source":"def statistics_rv_time_stock(df):\n    vr_cols = []\n    for col in df.columns:\n        if 'rv' in col:\n            vr_cols.append(col)\n\n    df_stock_id = df.groupby(['stock_id'])[vr_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    df_time_id = df.groupby(['time_id'])[vr_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    df = df.merge(df_stock_id, how = 'left', left_on = 'stock_id', right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = 'time_id', right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-15T03:20:05.437043Z","iopub.execute_input":"2021-08-15T03:20:05.437642Z","iopub.status.idle":"2021-08-15T03:20:05.448674Z","shell.execute_reply.started":"2021-08-15T03:20:05.437594Z","shell.execute_reply":"2021-08-15T03:20:05.446932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (2) Use **Target Encoding(TE)**\nWe can use the **mean value** of the **target volatility $\\sigma$** to represent the feature of **stock_id** (Because the same stock series has strong **autocorrelation**)  \n\n<font color=red>**Be ware:** Since the distribution of test set and training set may be different, we use **TE** with noise</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\ndef stock_id_TE(df_train,df_test):\n    if 'stock_id' not in df_train.columns:\n        df_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\n    if 'stock_id' not in df_test.columns:\n        df_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n    stock_id_target_mean = df_train.groupby('stock_id')['target'].mean()\n    df_test['stock_id_target_encoding'] = df_test['stock_id'].map(stock_id_target_mean)\n    # TE with noise in training set\n    tmp = np.repeat(np.nan, df_train.shape[0])\n    kf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\n    for idx_1, idx_2 in kf.split(df_train):\n        target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n        tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\n    df_train['stock_id_target_enc'] = tmp\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:20:06.188648Z","iopub.execute_input":"2021-08-15T03:20:06.189044Z","iopub.status.idle":"2021-08-15T03:20:07.174332Z","shell.execute_reply.started":"2021-08-15T03:20:06.189011Z","shell.execute_reply":"2021-08-15T03:20:07.173142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (3) Use **Embedding Layer**","metadata":{}},{"cell_type":"code","source":"hidden_units = (32,16)\nstock_embedding_size = 16\ndef Embedding_NN(train,hidden_units,stock_embedding_size,num_shape,cat_shape):\n    cat_data = train['stock_id']\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(3,), name='num_data')\n    \n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                            input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='selu')(out)\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n        inputs = [stock_id_input, num_input],\n        outputs = out,\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:20:07.345964Z","iopub.execute_input":"2021-08-15T03:20:07.346432Z","iopub.status.idle":"2021-08-15T03:20:07.355479Z","shell.execute_reply.started":"2021-08-15T03:20:07.346392Z","shell.execute_reply":"2021-08-15T03:20:07.354669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (4) Use **stock_id, time_id** as **category variables**","metadata":{}},{"cell_type":"code","source":"def category_stock(df_train,df_test):\n    df_train['stock_id'].astype('cat')\n    df_test['stock_id'].astype('cat')\n    return df_train,df_test","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:20:08.372375Z","iopub.execute_input":"2021-08-15T03:20:08.37294Z","iopub.status.idle":"2021-08-15T03:20:08.377453Z","shell.execute_reply.started":"2021-08-15T03:20:08.372907Z","shell.execute_reply":"2021-08-15T03:20:08.376574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"def feature_scale(df_train,df_test,features,method=None):\n    for f in features:\n        if method == 'min_max':\n            minimun = df_train[f].min()\n            maximum = df_train[f].max()\n            df_train[f] = (df_train[f] - minimum) / (maximum - minimum)\n            df_test[f] = (df_test[f] - minimum) / (maximum - minimum)\n        if method == 'z_score':\n            minimum = df_train[f].min()\n            std = df_train[f].std()\n            df_train[f] = (df_train[f] - minimum) / std\n            df_test[f] = (df_test[f] - minimum) / std\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:20:14.483446Z","iopub.execute_input":"2021-08-15T03:20:14.48406Z","iopub.status.idle":"2021-08-15T03:20:14.491549Z","shell.execute_reply.started":"2021-08-15T03:20:14.484007Z","shell.execute_reply":"2021-08-15T03:20:14.490349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Between Target and Feature","metadata":{}},{"cell_type":"markdown","source":"### Use **TE** to handling **stock_id, time_id**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest_stock_ids = test['stock_id'].unique()\ntest_feature = FeatureEngineering(test_stock_ids, is_train = False, use_numba=True)\ntrain_feature = pd.read_csv('../input/optiver-train-data/train_feature.csv')\ntrain_feature.fillna(method='ffill',inplace=True)\ntrain_feature.fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:20:18.333049Z","iopub.execute_input":"2021-08-15T03:20:18.333628Z","iopub.status.idle":"2021-08-15T03:21:20.14201Z","shell.execute_reply.started":"2021-08-15T03:20:18.333592Z","shell.execute_reply":"2021-08-15T03:21:20.140726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature['stock_id'] = train_feature['row_id'].apply(lambda x: x.split('-')[0]).astype('int64')\ntrain_feature['time_id'] = train_feature['row_id'].apply(lambda x: x.split('-')[1]).astype('int64')\ntrain_feature = train_feature.merge(train, how='left', on=['stock_id','time_id'])\ntest_feature = test_feature.merge(test, how='right', on=['row_id'])\ntest_feature.fillna(method='ffill',inplace=True)\ntest_feature.fillna(0,inplace=True)\ntrain_feature, test_feature = stock_id_TE(train_feature,test_feature)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:21:20.144456Z","iopub.execute_input":"2021-08-15T03:21:20.144863Z","iopub.status.idle":"2021-08-15T03:21:26.446295Z","shell.execute_reply.started":"2021-08-15T03:21:20.14482Z","shell.execute_reply":"2021-08-15T03:21:26.445185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fetch features' name","metadata":{}},{"cell_type":"code","source":"features = []\nfor col in train_feature.columns:\n    if col not in ['stock_id','time_id','row_id','target']:\n        features.append(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:21:26.448301Z","iopub.execute_input":"2021-08-15T03:21:26.448682Z","iopub.status.idle":"2021-08-15T03:21:26.453836Z","shell.execute_reply.started":"2021-08-15T03:21:26.448648Z","shell.execute_reply":"2021-08-15T03:21:26.45266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate correlations","metadata":{}},{"cell_type":"code","source":"corr_matrix = train_feature.corr()\ncorr_target = corr_matrix['target']\ncorr_target = corr_target.sort_values(ascending = False)\ncorr_target = corr_target.to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:21:26.455709Z","iopub.execute_input":"2021-08-15T03:21:26.456261Z","iopub.status.idle":"2021-08-15T03:22:35.75028Z","shell.execute_reply.started":"2021-08-15T03:21:26.456187Z","shell.execute_reply":"2021-08-15T03:22:35.749019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_target.head(500)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:22:35.751883Z","iopub.execute_input":"2021-08-15T03:22:35.752529Z","iopub.status.idle":"2021-08-15T03:22:35.797213Z","shell.execute_reply.started":"2021-08-15T03:22:35.752481Z","shell.execute_reply":"2021-08-15T03:22:35.796167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g1_features = []\nfor col in train_feature.columns:\n    if 'rv' in col:\n        g1_features.append(col)\ncorr_target.loc[g1_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:23:57.242257Z","iopub.execute_input":"2021-08-15T03:23:57.243061Z","iopub.status.idle":"2021-08-15T03:23:57.261114Z","shell.execute_reply.started":"2021-08-15T03:23:57.243004Z","shell.execute_reply":"2021-08-15T03:23:57.259978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g1_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:46:15.610569Z","iopub.execute_input":"2021-08-15T03:46:15.610988Z","iopub.status.idle":"2021-08-15T03:46:15.666306Z","shell.execute_reply.started":"2021-08-15T03:46:15.61095Z","shell.execute_reply":"2021-08-15T03:46:15.665202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g2_features = []\nfor col in train_feature.columns:\n    if 'std' in col and 'log_return' in col:\n        g2_features.append(col)\ncorr_target.loc[g2_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:24:39.759407Z","iopub.execute_input":"2021-08-15T03:24:39.759992Z","iopub.status.idle":"2021-08-15T03:24:39.774225Z","shell.execute_reply.started":"2021-08-15T03:24:39.759936Z","shell.execute_reply":"2021-08-15T03:24:39.772851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g2_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:46:09.251004Z","iopub.execute_input":"2021-08-15T03:46:09.251394Z","iopub.status.idle":"2021-08-15T03:46:09.30802Z","shell.execute_reply.started":"2021-08-15T03:46:09.251361Z","shell.execute_reply":"2021-08-15T03:46:09.306864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g3_features = []\nfor col in train_feature.columns:\n    if 'max' in col and 'log_return' in col:\n        g3_features.append(col)\ncorr_target.loc[g3_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:25:04.317866Z","iopub.execute_input":"2021-08-15T03:25:04.318291Z","iopub.status.idle":"2021-08-15T03:25:04.332392Z","shell.execute_reply.started":"2021-08-15T03:25:04.318254Z","shell.execute_reply":"2021-08-15T03:25:04.330765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g3_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:46:02.38042Z","iopub.execute_input":"2021-08-15T03:46:02.381006Z","iopub.status.idle":"2021-08-15T03:46:02.416316Z","shell.execute_reply.started":"2021-08-15T03:46:02.380954Z","shell.execute_reply":"2021-08-15T03:46:02.41496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g4_features = []\nfor col in train_feature.columns:\n    if 'min' in col and 'log_return' in col:\n        g4_features.append(col)\ncorr_target.loc[g4_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:25:29.568364Z","iopub.execute_input":"2021-08-15T03:25:29.568747Z","iopub.status.idle":"2021-08-15T03:25:29.5816Z","shell.execute_reply.started":"2021-08-15T03:25:29.568713Z","shell.execute_reply":"2021-08-15T03:25:29.580854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g4_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:53.61296Z","iopub.execute_input":"2021-08-15T03:45:53.613356Z","iopub.status.idle":"2021-08-15T03:45:53.654101Z","shell.execute_reply.started":"2021-08-15T03:45:53.613325Z","shell.execute_reply":"2021-08-15T03:45:53.652932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g5_features = []\nfor col in train_feature.columns:\n    if 'price_spread' in col and 'min' not in col:\n        g5_features.append(col)\ncorr_target.loc[g5_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:30:27.097137Z","iopub.execute_input":"2021-08-15T03:30:27.097606Z","iopub.status.idle":"2021-08-15T03:30:27.11246Z","shell.execute_reply.started":"2021-08-15T03:30:27.097546Z","shell.execute_reply":"2021-08-15T03:30:27.111395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g5_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:46.742662Z","iopub.execute_input":"2021-08-15T03:45:46.743056Z","iopub.status.idle":"2021-08-15T03:45:46.786208Z","shell.execute_reply.started":"2021-08-15T03:45:46.743024Z","shell.execute_reply":"2021-08-15T03:45:46.78497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g6_features = []\nfor col in train_feature.columns:\n    if 'wap_balance' in col and 'min' not in col:\n        g6_features.append(col)\ncorr_target.loc[g6_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:32:51.693634Z","iopub.execute_input":"2021-08-15T03:32:51.694107Z","iopub.status.idle":"2021-08-15T03:32:51.70888Z","shell.execute_reply.started":"2021-08-15T03:32:51.694055Z","shell.execute_reply":"2021-08-15T03:32:51.707791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g6_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:36.448926Z","iopub.execute_input":"2021-08-15T03:45:36.449327Z","iopub.status.idle":"2021-08-15T03:45:36.488467Z","shell.execute_reply.started":"2021-08-15T03:45:36.449294Z","shell.execute_reply":"2021-08-15T03:45:36.487198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g7_features = []\nfor col in train_feature.columns:\n    if 'wap' in col and 'std' in col and 'wap_balance' not in col:\n        g7_features.append(col)\ncorr_target.loc[g7_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:34:33.386292Z","iopub.execute_input":"2021-08-15T03:34:33.38695Z","iopub.status.idle":"2021-08-15T03:34:33.400134Z","shell.execute_reply.started":"2021-08-15T03:34:33.386913Z","shell.execute_reply":"2021-08-15T03:34:33.398988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g7_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:28.2403Z","iopub.execute_input":"2021-08-15T03:45:28.240699Z","iopub.status.idle":"2021-08-15T03:45:28.279567Z","shell.execute_reply.started":"2021-08-15T03:45:28.24066Z","shell.execute_reply":"2021-08-15T03:45:28.27799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g8_features = []\nfor col in train_feature.columns:\n    if 'bid_spread' in col and ('max' in col or 'std' in col):\n        g8_features.append(col)\ncorr_target.loc[g8_features,]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:13.121786Z","iopub.execute_input":"2021-08-15T03:45:13.12222Z","iopub.status.idle":"2021-08-15T03:45:13.137583Z","shell.execute_reply.started":"2021-08-15T03:45:13.122174Z","shell.execute_reply":"2021-08-15T03:45:13.135859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature[g8_features].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T03:45:16.810765Z","iopub.execute_input":"2021-08-15T03:45:16.811441Z","iopub.status.idle":"2021-08-15T03:45:16.845491Z","shell.execute_reply.started":"2021-08-15T03:45:16.811387Z","shell.execute_reply":"2021-08-15T03:45:16.843678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to plot featurn with **stock_id**","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\ndef plot_feature_with_stock(df,feature):\n    fig, ax = plt.subplots(16, 7, figsize=(20, 50))\n    ax = ax.flatten()\n    for i, stock_id in tqdm(enumerate(df['stock_id'].unique())):\n        ax[i].scatter(df.query('stock_id == @stock_id')[feature],\n                      df.query('stock_id == @stock_id')['target'],\n                      s=3,c='r',alpha=0.3)\n        ax[i].set_title(\"stock_id: \" + str(stock_id))\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:43:04.568065Z","iopub.execute_input":"2021-08-15T01:43:04.568524Z","iopub.status.idle":"2021-08-15T01:43:04.656501Z","shell.execute_reply.started":"2021-08-15T01:43:04.568485Z","shell.execute_reply":"2021-08-15T01:43:04.655265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <font color=red>Group according to different **stock_id** and **correlation coefficient** from high to low</font>  \n#### <font color=red>Plot the relationship between **feature** and **target**</font>","metadata":{}},{"cell_type":"code","source":"print(corr_target.index[1])\nplot_feature_with_stock(train_feature,corr_target.index[1])","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:43:06.28256Z","iopub.execute_input":"2021-08-15T01:43:06.284546Z","iopub.status.idle":"2021-08-15T01:43:28.92537Z","shell.execute_reply.started":"2021-08-15T01:43:06.283023Z","shell.execute_reply":"2021-08-15T01:43:28.923519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(corr_target.index[2])\nplot_feature_with_stock(train_feature,corr_target.index[2])","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:43:28.927348Z","iopub.execute_input":"2021-08-15T01:43:28.927678Z","iopub.status.idle":"2021-08-15T01:43:51.314021Z","shell.execute_reply.started":"2021-08-15T01:43:28.927646Z","shell.execute_reply":"2021-08-15T01:43:51.312784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(corr_target.index[240])\nplot_feature_with_stock(train_feature,corr_target.index[240])","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:43:51.316376Z","iopub.execute_input":"2021-08-15T01:43:51.316782Z","iopub.status.idle":"2021-08-15T01:44:13.306884Z","shell.execute_reply.started":"2021-08-15T01:43:51.31674Z","shell.execute_reply":"2021-08-15T01:44:13.305772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to plot **stock** with featurn","metadata":{}},{"cell_type":"code","source":"import matplotlib.colors as mcolors\ncolors = list(mcolors.TABLEAU_COLORS.keys())\ndef plot_stock_with_feature(df,stock_id):\n    fig, ax = plt.subplots(30, 8, figsize=(20, 80))\n    ax = ax.flatten()\n    for i in range(240):\n        ax[i].scatter(df.query('stock_id == @stock_id')[corr_target.index[i+1]],\n                      df.query('stock_id == @stock_id')['target'],\n                      s=3,c=mcolors.TABLEAU_COLORS[colors[i%10]],alpha=0.3)\n        ax[i].set_title(corr_target.index[i+1])\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:44:13.308566Z","iopub.execute_input":"2021-08-15T01:44:13.308905Z","iopub.status.idle":"2021-08-15T01:44:13.317392Z","shell.execute_reply.started":"2021-08-15T01:44:13.308869Z","shell.execute_reply":"2021-08-15T01:44:13.31628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_id = 0\nprint(\"stock_id: \",stock_id)\nplot_stock_with_feature(train_feature,stock_id)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:44:13.318961Z","iopub.execute_input":"2021-08-15T01:44:13.31934Z","iopub.status.idle":"2021-08-15T01:44:55.950355Z","shell.execute_reply.started":"2021-08-15T01:44:13.319304Z","shell.execute_reply":"2021-08-15T01:44:55.949271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_id = 1\nprint(\"stock_id: \",stock_id)\nplot_stock_with_feature(train_feature,stock_id)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:44:55.951602Z","iopub.execute_input":"2021-08-15T01:44:55.951901Z","iopub.status.idle":"2021-08-15T01:45:38.140444Z","shell.execute_reply.started":"2021-08-15T01:44:55.951872Z","shell.execute_reply":"2021-08-15T01:45:38.139302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=red>We can find some useful imformation from above</font>\n<font color=red>(1) The **mean value** of variables do not seem to have a linear correlation with the **target** </font>  \n<font color=red>(2) The **min value** of variables seem to have a negative linear correlation with the **target** </font>  \n<font color=red>(3) The **most important varialbes** might be **realized volatility** in the past and **statistics of log_return**. They all have a strong positive correlation with **target**</font>  \n<font color=red>(4) ......</font>","metadata":{}},{"cell_type":"markdown","source":"# Mutual Information","metadata":{}},{"cell_type":"markdown","source":"# FeatureGen","metadata":{}},{"cell_type":"markdown","source":"#### Now, we can package all functions related to feature engineering","metadata":{}},{"cell_type":"code","source":"def FeatureGen(train_load=False,use_TE=False,use_rv_statistics=False,use_category=False,use_scale=None):\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    train_stock_ids = train['stock_id'].unique()\n    test_stock_ids = test['stock_id'].unique()\n    # If train_data already saved, we can load it directly to save time in FeatureGen\n    if train_load:\n        train_feature = pd.read_csv('../input/optiver-train-data/train_feature.csv')\n    else:\n        train_feature = FeatureEngineering(train_stock_ids, is_train = True, use_numba=True)\n    test_feature = FeatureEngineering(test_stock_ids, is_train = False, use_numba=True)\n    \n    train_feature['stock_id'] = train_feature['row_id'].apply(lambda x: x.split('-')[0]).astype('int64')\n    train_feature['time_id'] = train_feature['row_id'].apply(lambda x: x.split('-')[1]).astype('int64')\n    train_feature = train_feature.merge(train, how='left', on=['stock_id','time_id'])\n    test_feature = test_feature.merge(test, how='right', on=['row_id'])\n    test_feature.fillna(method='ffill',inplace=True)\n    test_feature.fillna(0,inplace=True)\n    # Whether to use Target Encoding\n    if use_TE:\n        train_feature, test_feature = stock_id_TE(train_feature,test_feature)\n    # Whether to use realized volatility statistics\n    if use_rv_statistics:\n        train_feature = statistics_rv_time_stock(train_feature)\n        test_feature = statistics_rv_time_stock(test_feature)\n    # Whether to make stock_id be category variable\n    if use_category:\n        train_feature, test_feature = category_stock(train_feature,test_feature)\n    # Whether to use feature scaling\n    featurns_scaling = []\n    if use_scale != None:\n        train_feature, test_feature = feature_scale(train_feature,test_feature,features_scaling,method=use_scale)\n    \n    return train_feature, test_featurn","metadata":{},"execution_count":null,"outputs":[]}]}